### These resources I found useful are based on my EECS background. 
**algorithms**
 - [x] tree 
  - The Elements of Statistic Learning Chap. 9.2
  - [one implementation](https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/)
 - [x] gradient boost decision tree
  - [Greedy function approximation: A gradient boosting machine](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)
  - [Intro. BoostedTree.pdf](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)
 - [x] AdaBoost
 -[ML-notes-pp:61-66](http://www-m5.ma.tum.de/foswiki/pub/M5/Allgemeines/MA4801_2018S/ML_notes_main.pdf)
 - [x] gradient boosting
 - [ML-notes-pp:66-68](http://www-m5.ma.tum.de/foswiki/pub/M5/Allgemeines/MA4801_2018S/ML_notes_main.pdf)
 - [Greedy function approximation: A gradient boosting machine](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)
 - [wiki: Gradient_boosting](https://en.wikipedia.org/wiki/Gradient_boosting)
 - [Boosting algorithms as gradient descent](http://papers.nips.cc/paper/1766-boosting-algorithms-as-gradient-descent.pdf)
 - [Intro. gradient_boosting](http://www.chengli.io/tutorials/gradient_boosting.pdf)
 - [x] XGBoost
 - [a scalable tree boosting system](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)
 - [x] [Naive Bayesian clssification](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)
  - Gaussian naive Bayes or Gaussian discriminate analysis
  - multinomial naive Bayes
  - Bernoulli Bayes
  -   [likelihood estimation of multinomial distribution](https://math.stackexchange.com/questions/421105/maximum-likelihood-estimator-of-parameters-of-multinomial-distribution)
 - [x] [bagging, random forests, boosting](https://web.stanford.edu/class/stats202/content/lec20.pdf)
 - [x] svm
 - [guide to svms](http://web.mit.edu/6.034/wwwbob/svm.pdf)
 - [wiki: svm](https://en.wikipedia.org/wiki/Support-vector_machine)
 - [loss function: mse, entropy, hinge](https://rohanvarma.me/Loss-Functions/)
 - [wiki: loss function for classification](https://en.wikipedia.org/wiki/Loss_functions_for_classification)
 - [kernel trick](https://stats.stackexchange.com/questions/48506/what-function-could-be-a-kernel)
 - [common kernel functions](https://data-flair.training/blogs/svm-kernel-functions/)
 - [support vector regression](https://www.mathworks.com/help/stats/understanding-support-vector-machine-regression.html)
 - [svr tutor](https://alex.smola.org/papers/2003/SmoSch03b.pdf)
 - [x] linear regression 
  - [more detailed cases](http://statweb.stanford.edu/~owen/courses/305-1415/ch2.pdf)
  - [over and under determined system](http://people.csail.mit.edu/bkph/articles/Pseudo_Inverse.pdf)
  - [kernelized linear regression](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote13.html)
  - [kernals in regression](https://www.cse.iitk.ac.in/users/piyush/courses/ml_autumn18/material/771_A18_lec12_print.pdf)
 - [x] logistic regression
 - [x] lasso 
 - [x] principal component analysis (PCA)
 - [x] latent dirichlet allocation (LDA)
  -[original paper](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) 
  - [wiki](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) 
 - [x] independent component analysis (ICA)
 - [turorial](http://www.cs.jhu.edu/~ayuille/courses/Stat161-261-Spring14/HyvO00-icatut.pdf)
  - [wiki](https://en.wikipedia.org/wiki/Independent_component_analysis)
  - [cs229 lecture note](http://cs229.stanford.edu/summer2019/cs229-notes11.pdf)
 - [x] factor analysis
  - [cs229 lecture note](http://cs229.stanford.edu/summer2019/cs229-notes9.pdf)
 - [x] independent factor analysis
  - [independent factor analysis ](https://pdfs.semanticscholar.org/ea7a/c9237c240b86944d34dd93fbb5793b7f5437.pdf)
 - [x] k-means 
  -[wiki](https://en.wikipedia.org/wiki/K-means_clustering)
 - [x] Gaussian Process
  - [cs229 lecture note](http://cs229.stanford.edu/summer2019/gaussian_processes.pdf)
  - [the kernel cookbook](https://www.cs.toronto.edu/~duvenaud/cookbook/)
 - [x] generative models
  - [definitely should be the PRML](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
 - [x] anomaly detection 
  - [one class svm](https://papers.nips.cc/paper/1723-support-vector-method-for-novelty-detection.pdf)
  - [isolation forest](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest)
 - [x] Linear Discriminate Analysis(LDA)
 - [ ] semi-supervised learning
  - [Laplacian SVM](https://www.jmlr.org/papers/volume7/belkin06a/belkin06a.pdf)
 - [ ] [Bayesian Optimization](https://arxiv.org/pdf/1807.02811.pdf)
 - [ ] [Approximate Nearest Neibor Search](https://github.com/spotify/annoy)
 
 **Concepts**
 - [x] bias-variance trade-off 
  - [ML notes, pp 8--11](http://www-m5.ma.tum.de/foswiki/pub/M5/Allgemeines/MA4801_2018S/ML_notes_main.pdf)
  - [cs229 lecture note](http://cs229.stanford.edu/summer2019/BiasVarianceAnalysis.pdf)
 - [x] No free lunch
 - [x] [List of probability](https://en.wikipedia.org/wiki/List_of_probability_distributions)
 - [x] [Conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior)
 - [x] calculus of variations
 - [blog: the clculus of variation](http://bjlkeng.github.io/posts/the-calculus-of-variations/)
 - [x] ROC, PR 
  - [wiki: ROC, PR](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)
 - [x] [precision & recall](https://en.wikipedia.org/wiki/Precision_and_recall)
 - [x] matrix calculus
  - [matrix calculus: appendix D](https://www.cs.cmu.edu/~epxing/Class/10701-08s/recitation/mc.pdf)
  - [the matrix cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)
 - [x] EM & variational EM  
  - [The Variational Approximation for Bayesian Inference ](http://www.cs.uoi.gr/~arly/papers/SPM08.pdf)
  - [pattern recognition and machine learning, chapter 9, 10](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
   - [lecture note](http://cs229.stanford.edu/notes/cs229-notes8.pdf)

**reinforcement learning** 
- [x] [Sutton: reinforcement learning, an introduction](http://incompleteideas.net/book/the-book-2nd.html)

**deep learning**
- [x] Goodfellow I, Bengio Y, Courville A. "Deep learning". 
 - [Home page](https://www.deeplearningbook.org/) 
 - [github: pdf](https://github.com/janishar/mit-deep-learning-book-pdf)
 - [x] tutorial 
  - [http://ufldl.stanford.edu/tutorial/](http://ufldl.stanford.edu/tutorial/)
 - [ ] [stanford cs230](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)
 - [ ] [Bayesian Deep Networks](https://www.cs.tufts.edu/comp/150BDL/2019f/)
 - [x] [Analyses of Deep Learning (Stanford STATS 385)](https://stats385.github.io/)
- [ ] [Neural Networks and Deep Learning](https://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/)

**optimization**
- [x] convex optimization 
 - [stephen boyd, "convex optimization"](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
- [x] ADMM 
 - [stephen boyd, admm](https://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf)
 - [slides](https://web.stanford.edu/~boyd/papers/pdf/admm_slides.pdf)
- [x] EPFL Course - Optimization for Machine Learning - CS-439
 - [github](https://github.com/epfml/OptML_course)
 - [x] overview of gradient descent optimization algorithms
  - [blog: optimizing-gradient-descent](http://ruder.io/optimizing-gradient-descent/)
  - [overview of recent gd methods](https://johnchenresearch.github.io/demon/)
  
**Detection, Estimation and Hypothesis Testing**
- [x] [detection and estimation theory](https://www.ece.iastate.edu/~namrata/EE527/)
- [x] Hypothesis testing
 - [caltech](http://www.its.caltech.edu/~mshum/stats/lect8.pdf)
 - [tutor](http://personal.lse.ac.uk/PIFFER/HT%20for%20beginners.pdf) 
