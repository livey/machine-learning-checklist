* why not use moving average in batch normalization
  - [ ] batch differ, will result in large variance when updating the weight
  - [ ] in traditional BN, we backpropagate and count the input feature
* dropout 
  - [ ] improve generalization 
  - [ ] how it is worked - drop some input features with probability p, at inference time, use all but multiply by p for the dropout layers. 
* 
  
